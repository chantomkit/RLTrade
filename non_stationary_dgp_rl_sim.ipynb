{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from RLTrade.agent import DQNAgent, XGBoostAgent\n",
    "from RLTrade.utils import (\n",
    "    stationaryDGP, \n",
    "    nonstationaryDGP, \n",
    "    build_features, \n",
    "    build_rolling_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = nonstationaryDGP()\n",
    "spread = y - x\n",
    "df = pd.DataFrame({'x': x, 'y': y, 'close': spread}) # close price of portfolio is the spread\n",
    "df_train = df.copy()\n",
    "\n",
    "feature_config = {\n",
    "        \"feature_col\": \"close\",\n",
    "        \"window\": 7, # 7 days window, pad with first value when window is not available\n",
    "        \"mode\": \"diff\", # difference between current and previous close price\n",
    "        \"mean_correction\": True,\n",
    "    }\n",
    "df_train = build_rolling_feature(df_train, **feature_config)\n",
    "\n",
    "env_action_space = [-1, 0, 1] # Positions : [-1=SHORT, 0=OUT, 1=LONG]\n",
    "# env_action_space = [i/10 for i in range(-10, 11)] # Positions : [-1, -0.9, -0.8, ..., 0, 0.1, 0.2, ..., 1]\n",
    "\n",
    "env = gym.make(\"TradingEnv\",\n",
    "        name= \"stationaryDGP\",\n",
    "        df = df_train, # Your dataset with your custom features\n",
    "        positions = env_action_space, # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "        # max_episode_duration=180\n",
    "    )\n",
    "env.unwrapped.add_metric('Position Changes', lambda history : np.sum(np.diff(history['position']) != 0) )\n",
    "env.unwrapped.add_metric('Episode Length', lambda history : len(history['position']) )\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_observations = len(env.reset()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  2.36%   |   Portfolio Return : -57.85%   |   Position Changes : 5685   |   Episode Length : 10000   |   \n",
      "Market Return :  7.74%   |   Portfolio Return : -48.66%   |   Position Changes : 4993   |   Episode Length : 10000   |   \n",
      "Market Return : -7.25%   |   Portfolio Return : -42.90%   |   Position Changes : 4869   |   Episode Length : 10000   |   \n",
      "Market Return : -5.38%   |   Portfolio Return : -41.10%   |   Position Changes : 4659   |   Episode Length : 10000   |   \n",
      "Market Return : -10.74%   |   Portfolio Return : -42.78%   |   Position Changes : 4607   |   Episode Length : 10000   |   \n",
      "Market Return :  8.05%   |   Portfolio Return : -40.73%   |   Position Changes : 4544   |   Episode Length : 10000   |   \n",
      "Market Return :  3.49%   |   Portfolio Return : -39.97%   |   Position Changes : 4417   |   Episode Length : 10000   |   \n",
      "Market Return : -0.93%   |   Portfolio Return : -40.76%   |   Position Changes : 4150   |   Episode Length : 10000   |   \n",
      "Market Return : -9.91%   |   Portfolio Return : -31.39%   |   Position Changes : 4033   |   Episode Length : 10000   |   \n",
      "Market Return : -20.63%   |   Portfolio Return : -37.01%   |   Position Changes : 3837   |   Episode Length : 10000   |   \n",
      "Market Return : 11.75%   |   Portfolio Return : -28.24%   |   Position Changes : 3723   |   Episode Length : 10000   |   \n",
      "Market Return :  5.56%   |   Portfolio Return : -28.89%   |   Position Changes : 3499   |   Episode Length : 10000   |   \n",
      "Market Return :  8.69%   |   Portfolio Return : -38.27%   |   Position Changes : 3374   |   Episode Length : 10000   |   \n",
      "Market Return :  0.81%   |   Portfolio Return : -35.02%   |   Position Changes : 3080   |   Episode Length : 10000   |   \n",
      "Market Return :  6.40%   |   Portfolio Return : -32.61%   |   Position Changes : 3024   |   Episode Length : 10000   |   \n",
      "Market Return : -61.01%   |   Portfolio Return : -25.95%   |   Position Changes : 2868   |   Episode Length : 10000   |   \n",
      "Market Return : 11.49%   |   Portfolio Return : -25.53%   |   Position Changes : 2719   |   Episode Length : 10000   |   \n",
      "Market Return :  0.87%   |   Portfolio Return : -13.48%   |   Position Changes : 2539   |   Episode Length : 10000   |   \n",
      "Market Return :  7.01%   |   Portfolio Return : -28.25%   |   Position Changes : 2225   |   Episode Length : 10000   |   \n",
      "Market Return :  6.66%   |   Portfolio Return : -21.32%   |   Position Changes : 2011   |   Episode Length : 10000   |   \n",
      "Market Return : -23.74%   |   Portfolio Return : -21.17%   |   Position Changes : 1947   |   Episode Length : 10000   |   \n",
      "Market Return : 12.51%   |   Portfolio Return : -5.96%   |   Position Changes : 1663   |   Episode Length : 10000   |   \n",
      "Market Return :  4.10%   |   Portfolio Return : -13.53%   |   Position Changes : 1613   |   Episode Length : 10000   |   \n",
      "Market Return :  6.78%   |   Portfolio Return : -15.66%   |   Position Changes : 1480   |   Episode Length : 10000   |   \n",
      "Market Return :  0.47%   |   Portfolio Return : -11.71%   |   Position Changes : 1450   |   Episode Length : 10000   |   \n",
      "Market Return : -8.19%   |   Portfolio Return : -15.34%   |   Position Changes : 1368   |   Episode Length : 10000   |   \n",
      "Market Return :  9.76%   |   Portfolio Return : -12.25%   |   Position Changes : 1347   |   Episode Length : 10000   |   \n",
      "Market Return :  1.44%   |   Portfolio Return : -9.83%   |   Position Changes : 1319   |   Episode Length : 10000   |   \n",
      "Market Return : -24.71%   |   Portfolio Return : -12.48%   |   Position Changes : 1280   |   Episode Length : 10000   |   \n",
      "Market Return :  5.86%   |   Portfolio Return : -31.13%   |   Position Changes : 1285   |   Episode Length : 10000   |   \n",
      "Market Return : -24.49%   |   Portfolio Return : -6.07%   |   Position Changes : 1273   |   Episode Length : 10000   |   \n",
      "Market Return :  0.13%   |   Portfolio Return : -4.46%   |   Position Changes : 1288   |   Episode Length : 10000   |   \n",
      "Market Return :  3.31%   |   Portfolio Return : -30.32%   |   Position Changes : 1126   |   Episode Length : 10000   |   \n",
      "Market Return :  5.79%   |   Portfolio Return : -23.51%   |   Position Changes : 1260   |   Episode Length : 10000   |   \n",
      "Market Return : -12.46%   |   Portfolio Return : -11.96%   |   Position Changes : 1294   |   Episode Length : 10000   |   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m env\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39madd_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosition Changes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m history : np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mdiff(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) )\n\u001b[1;32m     21\u001b[0m env\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39madd_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode Length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m history : \u001b[38;5;28mlen\u001b[39m(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m'\u001b[39m]) )\n\u001b[0;32m---> 23\u001b[0m history_metrics, _ \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# One episode is one simulation\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/RLTrade/RLTrade/agent.py:134\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self, env, num_episodes)\u001b[0m\n\u001b[1;32m    131\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[1;32m    138\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "File \u001b[0;32m~/Projects/RLTrade/RLTrade/agent.py:89\u001b[0m, in \u001b[0;36mDQNAgent.optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m next_state_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBATCH_SIZE, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 89\u001b[0m     next_state_values[non_final_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_final_next_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Compute the expected Q values\u001b[39;00m\n\u001b[1;32m     91\u001b[0m expected_state_action_values \u001b[38;5;241m=\u001b[39m (next_state_values \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGAMMA) \u001b[38;5;241m+\u001b[39m reward_batch\n",
      "File \u001b[0;32m~/anaconda3/envs/env_full_py3_9/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env_full_py3_9/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/RLTrade/RLTrade/model.py:39\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x))\n\u001b[0;32m---> 39\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/env_full_py3_9/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env_full_py3_9/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env_full_py3_9/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train a DQN agent using training data from a stationary data generating process\n",
    "agent = DQNAgent(n_observations=n_observations, n_actions=n_actions)\n",
    "for i_simulation in range(100):\n",
    "    x, y = nonstationaryDGP()\n",
    "    spread = y - x\n",
    "    df = pd.DataFrame({'x': x, 'y': y, 'close': spread}) # close price of portfolio is the spread\n",
    "    df_train = df.copy()\n",
    "\n",
    "    df_train = build_rolling_feature(df_train, **feature_config)\n",
    "\n",
    "    # Define the environment\n",
    "    env = gym.make(\"TradingEnv\",\n",
    "            name= \"stationaryDGP\",\n",
    "            df = df_train, # Your dataset with your custom features\n",
    "            positions = env_action_space, # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "            trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "            borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "            # max_episode_duration=180\n",
    "        )\n",
    "    env.unwrapped.add_metric('Position Changes', lambda history : np.sum(np.diff(history['position']) != 0) )\n",
    "    env.unwrapped.add_metric('Episode Length', lambda history : len(history['position']) )\n",
    "\n",
    "    history_metrics, _ = agent.train(env, num_episodes=1) # One episode is one simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = nonstationaryDGP()\n",
    "spread = y - x\n",
    "df = pd.DataFrame({'x': x, 'y': y, 'close': spread}) # close price of portfolio is the spread\n",
    "df_train = df.copy()\n",
    "\n",
    "feature_config = {\n",
    "        \"feature_col\": \"close\",\n",
    "        \"window\": 100, # 7 days window, pad with first value when window is not available\n",
    "        \"mode\": \"diff\", # difference between current and previous close price\n",
    "        \"mean_correction\": True,\n",
    "    }\n",
    "df_train = build_rolling_feature(df_train, **feature_config)\n",
    "df_train = df_train.drop(columns=[f\"feature_rolling_{i}\" for i in range(1, 100)])\n",
    "\n",
    "env_action_space = [-1, 0, 1] # Positions : [-1=SHORT, 0=OUT, 1=LONG]\n",
    "# env_action_space = [i/10 for i in range(-10, 11)] # Positions : [-1, -0.9, -0.8, ..., 0, 0.1, 0.2, ..., 1]\n",
    "\n",
    "env = gym.make(\"TradingEnv\",\n",
    "        name= \"stationaryDGP\",\n",
    "        df = df_train, # Your dataset with your custom features\n",
    "        positions = env_action_space, # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "        # max_episode_duration=180\n",
    "    )\n",
    "env.unwrapped.add_metric('Position Changes', lambda history : np.sum(np.diff(history['position']) != 0) )\n",
    "env.unwrapped.add_metric('Episode Length', lambda history : len(history['position']) )\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_observations = len(env.reset()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : -16.40%   |   Portfolio Return : -69.78%   |   Position Changes : 6651   |   Episode Length : 10000   |   \n",
      "Market Return : -4.15%   |   Portfolio Return : -53.10%   |   Position Changes : 5653   |   Episode Length : 10000   |   \n",
      "Market Return :  0.61%   |   Portfolio Return : -33.51%   |   Position Changes : 3545   |   Episode Length : 10000   |   \n",
      "Market Return : 12.32%   |   Portfolio Return : -21.38%   |   Position Changes : 1671   |   Episode Length : 10000   |   \n",
      "Market Return : -10.05%   |   Portfolio Return : -13.25%   |   Position Changes : 963   |   Episode Length : 10000   |   \n",
      "Market Return : -7.67%   |   Portfolio Return : -4.29%   |   Position Changes : 549   |   Episode Length : 10000   |   \n",
      "Market Return :  4.72%   |   Portfolio Return : -2.00%   |   Position Changes : 348   |   Episode Length : 10000   |   \n",
      "Market Return :  6.37%   |   Portfolio Return : -0.14%   |   Position Changes : 244   |   Episode Length : 10000   |   \n",
      "Market Return : 15.65%   |   Portfolio Return : -1.29%   |   Position Changes : 196   |   Episode Length : 10000   |   \n",
      "Market Return : 21.76%   |   Portfolio Return : -1.60%   |   Position Changes : 214   |   Episode Length : 10000   |   \n",
      "Market Return : 12.78%   |   Portfolio Return : -0.54%   |   Position Changes : 177   |   Episode Length : 10000   |   \n",
      "Market Return :  6.81%   |   Portfolio Return : -0.16%   |   Position Changes : 172   |   Episode Length : 10000   |   \n",
      "Market Return : -13.76%   |   Portfolio Return : -2.48%   |   Position Changes : 174   |   Episode Length : 10000   |   \n",
      "Market Return : 10.53%   |   Portfolio Return :  0.29%   |   Position Changes : 140   |   Episode Length : 10000   |   \n",
      "Market Return : -3.95%   |   Portfolio Return : -1.18%   |   Position Changes : 166   |   Episode Length : 10000   |   \n",
      "Market Return :  6.04%   |   Portfolio Return : -1.66%   |   Position Changes : 143   |   Episode Length : 10000   |   \n",
      "Market Return : -27.36%   |   Portfolio Return :  8.57%   |   Position Changes : 158   |   Episode Length : 10000   |   \n",
      "Market Return : -8.79%   |   Portfolio Return :  1.69%   |   Position Changes : 191   |   Episode Length : 10000   |   \n",
      "Market Return : -3.77%   |   Portfolio Return : -0.44%   |   Position Changes : 151   |   Episode Length : 10000   |   \n",
      "Market Return : 12.90%   |   Portfolio Return : -1.90%   |   Position Changes : 165   |   Episode Length : 10000   |   \n",
      "Market Return : 16.43%   |   Portfolio Return : -2.45%   |   Position Changes : 174   |   Episode Length : 10000   |   \n",
      "Market Return : -12.49%   |   Portfolio Return : -1.17%   |   Position Changes : 125   |   Episode Length : 10000   |   \n",
      "Market Return : -17.08%   |   Portfolio Return : -0.74%   |   Position Changes : 135   |   Episode Length : 10000   |   \n",
      "Market Return :  5.37%   |   Portfolio Return : -0.75%   |   Position Changes : 202   |   Episode Length : 10000   |   \n",
      "Market Return : -6.47%   |   Portfolio Return :  0.45%   |   Position Changes : 166   |   Episode Length : 10000   |   \n",
      "Market Return :  7.42%   |   Portfolio Return : -2.24%   |   Position Changes : 161   |   Episode Length : 10000   |   \n",
      "Market Return : -19.61%   |   Portfolio Return : -1.65%   |   Position Changes : 198   |   Episode Length : 10000   |   \n",
      "Market Return : -15.17%   |   Portfolio Return :  0.08%   |   Position Changes : 155   |   Episode Length : 10000   |   \n",
      "Market Return :  9.60%   |   Portfolio Return : -3.80%   |   Position Changes : 149   |   Episode Length : 10000   |   \n",
      "Market Return : -9.55%   |   Portfolio Return : -1.43%   |   Position Changes : 132   |   Episode Length : 10000   |   \n",
      "Market Return : -21.67%   |   Portfolio Return :  0.03%   |   Position Changes : 171   |   Episode Length : 10000   |   \n",
      "Market Return : -1.81%   |   Portfolio Return : -0.96%   |   Position Changes : 139   |   Episode Length : 10000   |   \n",
      "Market Return : -13.91%   |   Portfolio Return : -0.45%   |   Position Changes : 155   |   Episode Length : 10000   |   \n",
      "Market Return : -5.68%   |   Portfolio Return : -1.65%   |   Position Changes : 131   |   Episode Length : 10000   |   \n",
      "Market Return : -19.84%   |   Portfolio Return :  0.50%   |   Position Changes : 140   |   Episode Length : 10000   |   \n",
      "Market Return : 12.35%   |   Portfolio Return : -1.04%   |   Position Changes : 150   |   Episode Length : 10000   |   \n",
      "Market Return : 17.57%   |   Portfolio Return : -2.75%   |   Position Changes : 155   |   Episode Length : 10000   |   \n",
      "Market Return : 16.52%   |   Portfolio Return : -1.45%   |   Position Changes : 144   |   Episode Length : 10000   |   \n",
      "Market Return :  7.36%   |   Portfolio Return : -0.51%   |   Position Changes : 149   |   Episode Length : 10000   |   \n",
      "Market Return : -5.17%   |   Portfolio Return : -0.47%   |   Position Changes : 147   |   Episode Length : 10000   |   \n",
      "Market Return : -5.87%   |   Portfolio Return :  0.42%   |   Position Changes : 169   |   Episode Length : 10000   |   \n",
      "Market Return : -0.11%   |   Portfolio Return :  0.13%   |   Position Changes : 124   |   Episode Length : 10000   |   \n",
      "Market Return : -21.37%   |   Portfolio Return : -0.71%   |   Position Changes : 188   |   Episode Length : 10000   |   \n",
      "Market Return : 18.00%   |   Portfolio Return : -0.05%   |   Position Changes : 158   |   Episode Length : 10000   |   \n",
      "Market Return : 106.07%   |   Portfolio Return : -6.13%   |   Position Changes : 260   |   Episode Length : 10000   |   \n",
      "Market Return : -1.86%   |   Portfolio Return :  1.08%   |   Position Changes : 176   |   Episode Length : 10000   |   \n",
      "Market Return : -0.25%   |   Portfolio Return : -0.55%   |   Position Changes : 149   |   Episode Length : 10000   |   \n",
      "Market Return :  0.52%   |   Portfolio Return :  0.19%   |   Position Changes : 163   |   Episode Length : 10000   |   \n",
      "Market Return : -3.09%   |   Portfolio Return : -3.94%   |   Position Changes : 147   |   Episode Length : 10000   |   \n",
      "Market Return :  5.95%   |   Portfolio Return :  0.72%   |   Position Changes : 147   |   Episode Length : 10000   |   \n",
      "Market Return : -10.71%   |   Portfolio Return : -0.64%   |   Position Changes : 160   |   Episode Length : 10000   |   \n",
      "Market Return :  4.09%   |   Portfolio Return : -3.47%   |   Position Changes : 237   |   Episode Length : 10000   |   \n",
      "Market Return : -9.85%   |   Portfolio Return : 10.60%   |   Position Changes : 6   |   Episode Length : 10000   |   \n",
      "Market Return :  5.44%   |   Portfolio Return : -6.92%   |   Position Changes : 388   |   Episode Length : 10000   |   \n",
      "Market Return : -5.84%   |   Portfolio Return : -13.61%   |   Position Changes : 1305   |   Episode Length : 10000   |   \n",
      "Market Return : -2.05%   |   Portfolio Return : -0.46%   |   Position Changes : 205   |   Episode Length : 10000   |   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m env\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39madd_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosition Changes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m history : np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mdiff(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) )\n\u001b[1;32m     22\u001b[0m env\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39madd_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode Length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m history : \u001b[38;5;28mlen\u001b[39m(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m'\u001b[39m]) )\n\u001b[0;32m---> 24\u001b[0m history_metrics, _ \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# One episode is one simulation\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/RLTrade/RLTrade/agent.py:253\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, env, num_episodes, replay_every)\u001b[0m\n\u001b[1;32m    251\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    252\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[1;32m    254\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m    255\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/envs/env_full_py3_9/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env_full_py3_9/lib/python3.9/site-packages/gym_trading_env/environments.py:239\u001b[0m, in \u001b[0;36mTradingEnv.step\u001b[0;34m(self, position_index)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions[position_index])\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_action_order_limit()\n\u001b[1;32m    242\u001b[0m price \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_price()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train a DQN agent using training data from a stationary data generating process\n",
    "agent = XGBoostAgent(n_observations, n_actions)\n",
    "for i_simulation in range(100):\n",
    "    x, y = nonstationaryDGP()\n",
    "    spread = y - x\n",
    "    df = pd.DataFrame({'x': x, 'y': y, 'close': spread}) # close price of portfolio is the spread\n",
    "    df_train = df.copy()\n",
    "\n",
    "    df_train = build_rolling_feature(df_train, **feature_config)\n",
    "    df_train = df_train.drop(columns=[f\"feature_rolling_{i}\" for i in range(1, 100)])\n",
    "\n",
    "    # Define the environment\n",
    "    env = gym.make(\"TradingEnv\",\n",
    "            name= \"stationaryDGP\",\n",
    "            df = df_train, # Your dataset with your custom features\n",
    "            positions = env_action_space, # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "            trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "            borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "            # max_episode_duration=180\n",
    "        )\n",
    "    env.unwrapped.add_metric('Position Changes', lambda history : np.sum(np.diff(history['position']) != 0) )\n",
    "    env.unwrapped.add_metric('Episode Length', lambda history : len(history['position']) )\n",
    "\n",
    "    history_metrics, _ = agent.train(env, num_episodes=1, replay_every=1) # One episode is one simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_full_py3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
